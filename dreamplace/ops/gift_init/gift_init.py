#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File              : gift_init.py
# Author            : Yiting Liu, Yibo Lin <yibolin@pku.edu.cn>
# Date              : 12.16.2024
# Last Modified Date: 12.17.2024
# Last Modified By  : Yibo Lin <yibolin@pku.edu.cn>

import torch
import torch.nn.functional as F
from torch import nn
import numpy as np
import scipy
import dreamplace.ops.gift_init.utils_gpu.util as util
import dreamplace.ops.gift_init.utils_gpu.mix_frequency_filter as mix_frequency_filter

import dreamplace.ops.gift_init.gift_init_cpp as gift_init_cpp

import logging
logger = logging.getLogger(__name__)

import time
import pdb

class GiFtInit(nn.Module):
    """ 
    @brief Compute initial position using GiFt technique published at ICCAD 2024. 
    Yiting Liu et al., The Power of Graph Signal Processing for Chip Placement Acceleration, ICCAD 2024. 
    """
    def __init__(self,
                 flat_netpin,
                 netpin_start,
                 pin2node_map,
                 net_weights,
                 net_mask, 
                 xl, yl, xh, yh,
                 num_nodes,
                 num_movable_nodes,
                 scale=0.7,
                 node_size_x=None,
                 node_size_y=None,
                 rass_options=None):
        """
        @brief initialization 
        @param flat_netpin flat netpin map, length of #pins 
        @param netpin_start starting index in netpin map for each net, length of #nets+1, the last entry is #pins  
        @param pin2node_map pin2node map 
        @param net_weights weight of nets 
        @param net_mask whether to compute wirelength, 1 means to compute, 0 means to ignore; users should guarantee invalid nets are filtered out  
        @param scale the distribution range of the initial locations generated by GiFt (customizable)
        """
        super(GiFtInit, self).__init__()

        self.num_nodes = num_nodes
        self.num_movable_nodes = num_movable_nodes
        self.num_fixed_nodes = num_nodes - num_movable_nodes
        self.xl = xl
        self.yl = yl
        self.xh = xh
        self.yh = yh
        self.scale = scale

        self.node_size_x = node_size_x.detach().clone() if node_size_x is not None else None
        self.node_size_y = node_size_y.detach().clone() if node_size_y is not None else None
        if self.node_size_x is not None:
            self.node_size_x_cpu = self.node_size_x.cpu().numpy()
            self.node_size_y_cpu = self.node_size_y.cpu().numpy()
        else:
            self.node_size_x_cpu = None
            self.node_size_y_cpu = None

        self.flat_netpin_cpu = flat_netpin.cpu().numpy()
        self.netpin_start_cpu = netpin_start.cpu().numpy()
        self.pin2node_cpu = pin2node_map.cpu().numpy()
        self.net_mask_cpu = net_mask.cpu().numpy().astype(bool)
        self.valid_net_indices = np.nonzero(self.net_mask_cpu)[0]
        self.hpwl_sample_count = min(5000, len(self.valid_net_indices))
        rng = np.random.default_rng(seed=0)
        if len(self.valid_net_indices) > self.hpwl_sample_count:
            self.hpwl_sample_indices = rng.choice(self.valid_net_indices, self.hpwl_sample_count, replace=False)
        else:
            self.hpwl_sample_indices = self.valid_net_indices

        self.rass = rass_options or {}
        self.rass_enabled = bool(self.rass.get("enabled", False) and self.rass.get("risk_map") is not None)
        if self.rass_enabled:
            self._setup_rass()

        logger.info('Construct adjacency matrix using clique model')

        ret = gift_init_cpp.adj_matrix_forward(flat_netpin.cpu(), netpin_start.cpu(), pin2node_map.cpu(), net_weights.cpu(), net_mask.cpu(), num_nodes)
        data = ret[0]
        row = ret[1]
        col = ret[2]
        dtype = np.float32 if net_weights.dtype == torch.float32 else np.float64
        self.adj_mat = scipy.sparse.coo_matrix((data.numpy(), (row.numpy(), col.numpy())), dtype=dtype, shape=(num_nodes, num_nodes))

        logger.info('Done matrix construction')

    def forward(self, pos):
        with torch.no_grad():
            pos_device = pos.device
            dtype = pos.dtype

            pos_t = pos.view([2, -1]).t().cpu().numpy()
            fixed_cell_location = pos_t[self.num_movable_nodes:self.num_movable_nodes + self.num_fixed_nodes]
            random_initial = util.generate_initial_locations(
                fixed_cell_location,
                self.num_movable_nodes,
                self.xl,
                self.yl,
                self.xh,
                self.yh,
                self.scale,
            )
            random_initial = np.concatenate((random_initial, fixed_cell_location), 0)
            random_initial = torch.from_numpy(random_initial).to(pos_device, dtype=dtype)

            start = time.time()
            low_pass_filter = mix_frequency_filter.GiFt_GPU(self.adj_mat, pos.device)
            low_pass_filter.train(4)
            location_low = low_pass_filter.get_cell_position(4, random_initial)

            low_pass_filter.train(4)
            location_mid = low_pass_filter.get_cell_position(2, random_initial)

            low_pass_filter.train(2)
            location_high = low_pass_filter.get_cell_position(2, random_initial)
            logger.info('GiFt filtering took %g sec', time.time() - start)

            bases = [location_low, location_mid, location_high, random_initial]
            baseline_weights = torch.tensor([0.2, 0.7, 0.1, 0.0], device=pos_device, dtype=dtype)

            if self.rass_enabled:
                risk_basis = self._risk_push_basis(location_mid)
                bases.append(risk_basis)
                baseline_weights = torch.cat(
                    [baseline_weights, torch.zeros(1, device=pos_device, dtype=dtype)], dim=0
                )
                blended, _ = self._select_rass_weights(bases, baseline_weights)
            else:
                blended = self._blend_bases(bases, baseline_weights)

            if self.num_fixed_nodes > 0:
                fixed_tensor = torch.from_numpy(fixed_cell_location).to(pos_device, dtype=dtype)
                blended[self.num_movable_nodes : self.num_movable_nodes + self.num_fixed_nodes] = fixed_tensor

            return blended.t()

    def _setup_rass(self):
        risk_map = self.rass["risk_map"].detach()
        self.rass["risk_map"] = risk_map
        self.rass["risk_map_cpu"] = risk_map.cpu()
        self.rass["risk_map_device"] = risk_map.device
        num_bins_x = self.rass.get("num_bins_x", risk_map.shape[0])
        num_bins_y = self.rass.get("num_bins_y", risk_map.shape[1])
        self.rass["num_bins_x"] = num_bins_x
        self.rass["num_bins_y"] = num_bins_y
        self.rass["bin_size_x"] = float(self.rass.get("bin_size_x", (self.xh - self.xl) / max(num_bins_x, 1)))
        self.rass["bin_size_y"] = float(self.rass.get("bin_size_y", (self.yh - self.yl) / max(num_bins_y, 1)))
        self.rass["xl"] = float(self.rass.get("xl", self.xl))
        self.rass["yl"] = float(self.rass.get("yl", self.yl))
        self.rass["threshold"] = float(self.rass.get("threshold", 0.7))
        self.rass["hpwl_guard"] = float(self.rass.get("hpwl_guard", 0.015))
        self.rass["disp_avg_guard"] = float(self.rass.get("disp_avg_guard", 0.02))
        self.rass["disp_max_guard"] = float(self.rass.get("disp_max_guard", 0.05))
        self.rass["num_samples"] = int(max(self.rass.get("num_samples", 0), 0))
        self.rass["adapt_flag"] = bool(self.rass.get("adapt_flag", True))
        self.rass["risk_weight"] = float(max(self.rass.get("risk_weight", 0.1), 1e-3))
        self.rass["layout_diag"] = float(max(self.rass.get("layout_diag", 1.0), 1e-6))
        self.rass["num_movable"] = int(self.rass.get("num_movable", self.num_movable_nodes))
        self.rass["eps"] = 1e-6

    def _blend_bases(self, bases, weights):
        stacked = torch.stack(bases, dim=0)
        weights = weights.view(-1, 1, 1)
        return torch.sum(stacked * weights, dim=0)

    def _select_rass_weights(self, bases, baseline_weights):
        device = bases[0].device
        dtype = bases[0].dtype
        num_bases = len(bases)
        base_w = baseline_weights
        if base_w.numel() < num_bases:
            pad = torch.zeros(num_bases - base_w.numel(), device=device, dtype=dtype)
            base_w = torch.cat([base_w, pad], dim=0)
        weights_candidates = [base_w]

        if not self.rass.get("adapt_flag", True):
            risk_focus = base_w.clone()
            if num_bases > 4:
                emphasis = 0.3
                risk_focus[-1] = emphasis
                remaining = max(1.0 - emphasis, 1e-6)
                core = base_w[:-1]
                core_sum = core.sum().item()
                if core_sum > 0:
                    risk_focus[:-1] = core * (remaining / core_sum)
                else:
                    risk_focus[:-1] = remaining / (num_bases - 1)
            weights_candidates.append(risk_focus)
        else:
            for i in range(num_bases):
                one_hot = torch.zeros(num_bases, device=device, dtype=dtype)
                one_hot[i] = 1.0
                weights_candidates.append(one_hot)
            alpha = np.ones(num_bases)
            for _ in range(self.rass.get("num_samples", 0)):
                sample = torch.from_numpy(np.random.dirichlet(alpha)).to(device=device, dtype=dtype)
                weights_candidates.append(sample)

        baseline_loc = self._blend_bases(bases, base_w)
        baseline_hpwl, baseline_risk, _, _ = self._evaluate_candidate(baseline_loc, baseline_loc)
        best_weights = base_w
        best_loc = baseline_loc
        best_score = float("inf")

        for weights in weights_candidates:
            loc = self._blend_bases(bases, weights)
            hpwl, risk, avg_disp, max_disp = self._evaluate_candidate(loc, baseline_loc)
            if not self._passes_guards(hpwl, baseline_hpwl, avg_disp, max_disp):
                continue
            score = self._score_candidate(hpwl, baseline_hpwl, risk, baseline_risk)
            if score < best_score:
                best_score = score
                best_weights = weights
                best_loc = loc

        if best_score == float("inf"):
            return baseline_loc, base_w

        return best_loc, best_weights

    def _evaluate_candidate(self, loc, baseline_loc):
        hpwl = self._approx_hpwl(loc)
        risk = self._avg_risk(loc)
        disp = torch.norm(loc - baseline_loc, dim=1)
        avg_disp = disp.mean().item()
        max_disp = disp.max().item()
        return hpwl, risk, avg_disp, max_disp

    def _passes_guards(self, hpwl, baseline_hpwl, avg_disp, max_disp):
        diag = self.rass["layout_diag"]
        hpwl_guard = self.rass["hpwl_guard"]
        avg_guard = diag * self.rass["disp_avg_guard"]
        max_guard = diag * self.rass["disp_max_guard"]
        hpwl_ok = baseline_hpwl <= 0 or hpwl <= baseline_hpwl * (1.0 + hpwl_guard + 1e-9)
        return hpwl_ok and avg_disp <= avg_guard and max_disp <= max_guard

    def _score_candidate(self, hpwl, baseline_hpwl, risk, baseline_risk):
        hpwl_ratio = hpwl / baseline_hpwl if baseline_hpwl > 0 else hpwl
        if baseline_risk > 1e-9:
            risk_ratio = risk / baseline_risk
        else:
            risk_ratio = risk
        return hpwl_ratio + self.rass["risk_weight"] * risk_ratio

    def _risk_push_basis(self, base):
        centers = self._centers(base)
        risk_vals = self._risk_sample(centers)
        threshold = self.rass["threshold"]
        severity = torch.clamp(risk_vals - threshold, min=0.0)
        if severity.max() > 0:
            severity = severity / severity.max()
        grad_x, grad_y = self._risk_gradient(centers)
        grads = torch.stack([grad_x, grad_y], dim=1)
        step = severity.unsqueeze(1) * grads
        max_step = torch.tensor(
            [self.rass["bin_size_x"], self.rass["bin_size_y"]],
            device=base.device,
            dtype=base.dtype,
        ) * 0.5
        step = torch.clamp(step, min=-max_step, max=max_step)
        return base - step

    def _centers(self, coords):
        if self.node_size_x is None:
            return coords
        size_x = self.node_size_x.to(coords.device)
        size_y = self.node_size_y.to(coords.device)
        centers = torch.empty_like(coords)
        centers[:, 0] = coords[:, 0] + 0.5 * size_x
        centers[:, 1] = coords[:, 1] + 0.5 * size_y
        return centers

    def _risk_sample(self, coords):
        risk_map = self.rass["risk_map"]
        if risk_map.device != coords.device:
            risk_map = risk_map.to(coords.device)
        xl = coords.new_tensor(self.rass["xl"])
        yl = coords.new_tensor(self.rass["yl"])
        bin_size_x = coords.new_tensor(self.rass["bin_size_x"])
        bin_size_y = coords.new_tensor(self.rass["bin_size_y"])
        fx = (coords[:, 0] - xl) / bin_size_x
        fy = (coords[:, 1] - yl) / bin_size_y
        max_x = coords.new_tensor(self.rass["num_bins_x"] - 1 - self.rass["eps"])
        max_y = coords.new_tensor(self.rass["num_bins_y"] - 1 - self.rass["eps"])
        fx = torch.clamp(fx, min=0.0)
        fy = torch.clamp(fy, min=0.0)
        fx = torch.minimum(fx, max_x)
        fy = torch.minimum(fy, max_y)
        ix0 = torch.floor(fx)
        iy0 = torch.floor(fy)
        tx = fx - ix0
        ty = fy - iy0
        ix0 = ix0.long()
        iy0 = iy0.long()
        ix1 = torch.clamp(ix0 + 1, max=self.rass["num_bins_x"] - 1)
        iy1 = torch.clamp(iy0 + 1, max=self.rass["num_bins_y"] - 1)
        r00 = risk_map[ix0, iy0]
        r10 = risk_map[ix1, iy0]
        r01 = risk_map[ix0, iy1]
        r11 = risk_map[ix1, iy1]
        risk_val = (1.0 - tx) * (1.0 - ty) * r00 \
            + tx * (1.0 - ty) * r10 \
            + (1.0 - tx) * ty * r01 \
            + tx * ty * r11
        return risk_val

    def _risk_gradient(self, centers):
        bin_size_x = centers.new_tensor(self.rass["bin_size_x"])
        bin_size_y = centers.new_tensor(self.rass["bin_size_y"])
        plus_x = centers.clone()
        plus_x[:, 0] += bin_size_x
        minus_x = centers.clone()
        minus_x[:, 0] -= bin_size_x
        plus_y = centers.clone()
        plus_y[:, 1] += bin_size_y
        minus_y = centers.clone()
        minus_y[:, 1] -= bin_size_y
        r_plus_x = self._risk_sample(plus_x)
        r_minus_x = self._risk_sample(minus_x)
        r_plus_y = self._risk_sample(plus_y)
        r_minus_y = self._risk_sample(minus_y)
        grad_x = (r_plus_x - r_minus_x) / (2.0 * bin_size_x)
        grad_y = (r_plus_y - r_minus_y) / (2.0 * bin_size_y)
        return grad_x, grad_y

    def _avg_risk(self, loc):
        if not self.rass_enabled:
            return 0.0
        centers = self._centers(loc)
        risk_vals = self._risk_sample(centers)
        num_movable = self.rass.get("num_movable", self.num_movable_nodes)
        if num_movable <= 0:
            return risk_vals.mean().item()
        return risk_vals[:num_movable].mean().item()

    def _approx_hpwl(self, loc):
        if len(self.hpwl_sample_indices) == 0:
            return 0.0
        loc_np = loc.detach().cpu().numpy()
        x = loc_np[:, 0]
        y = loc_np[:, 1]
        total = 0.0
        for net in self.hpwl_sample_indices:
            start = self.netpin_start_cpu[net]
            end = self.netpin_start_cpu[net + 1]
            pins = self.flat_netpin_cpu[start:end]
            if pins.size == 0:
                continue
            nodes = self.pin2node_cpu[pins]
            xs = x[nodes]
            ys = y[nodes]
            span_x = xs.max() - xs.min()
            span_y = ys.max() - ys.min()
            total += span_x + span_y
        return total / max(len(self.hpwl_sample_indices), 1)
